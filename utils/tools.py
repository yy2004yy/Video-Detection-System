"""
è®¾ç½®éšæœºç§å­ã€ä¿å­˜æ¨¡åž‹ç­‰å·¥å…·å‡½æ•°
"""
import os
import random
import numpy as np
import torch
from pathlib import Path
from typing import Optional, Dict, Any
import yaml


def set_seed(seed: int = 42):
    """
    è®¾ç½®éšæœºç§å­ï¼Œç¡®ä¿å®žéªŒå¯å¤çŽ°
    
    Args:
        seed: éšæœºç§å­å€¼
    """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


def save_checkpoint(state: Dict[str, Any],
                   checkpoint_dir: str,
                   filename: str = "checkpoint.pth",
                   is_best: bool = False):
    """
    ä¿å­˜æ¨¡åž‹æ£€æŸ¥ç‚¹
    
    Args:
        state: è¦ä¿å­˜çš„çŠ¶æ€å­—å…¸ï¼ˆåŒ…å«model_state_dict, optimizer_state_dictç­‰ï¼‰
        checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        filename: æ–‡ä»¶å
        is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡åž‹
    """
    os.makedirs(checkpoint_dir, exist_ok=True)
    
    # ä¿å­˜å¸¸è§„æ£€æŸ¥ç‚¹
    filepath = os.path.join(checkpoint_dir, filename)
    torch.save(state, filepath)
    
    # å¦‚æžœæ˜¯æœ€ä½³æ¨¡åž‹ï¼Œé¢å¤–ä¿å­˜
    if is_best:
        best_filepath = os.path.join(checkpoint_dir, "best_model.pth")
        torch.save(state, best_filepath)
        print(f"âœ… ä¿å­˜æœ€ä½³æ¨¡åž‹åˆ°: {best_filepath}")


def load_checkpoint(checkpoint_path: str,
                   model: Optional[torch.nn.Module] = None,
                   optimizer: Optional[torch.optim.Optimizer] = None,
                   device: Optional[str] = None) -> Dict[str, Any]:
    """
    åŠ è½½æ¨¡åž‹æ£€æŸ¥ç‚¹
    
    Args:
        checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
        model: è¦åŠ è½½æƒé‡çš„æ¨¡åž‹ï¼ˆå¯é€‰ï¼‰
        optimizer: è¦åŠ è½½çŠ¶æ€çš„ä¼˜åŒ–å™¨ï¼ˆå¯é€‰ï¼‰
        device: è®¾å¤‡ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        Dict[str, Any]: åŠ è½½çš„çŠ¶æ€å­—å…¸
    """
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    checkpoint = torch.load(checkpoint_path, map_location=device)
    
    if model is not None and 'model_state_dict' in checkpoint:
        model.load_state_dict(checkpoint['model_state_dict'])
        print(f"âœ… æ¨¡åž‹æƒé‡å·²åŠ è½½")
    
    if optimizer is not None and 'optimizer_state_dict' in checkpoint:
        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        print(f"âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²åŠ è½½")
    
    return checkpoint


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½YAMLé…ç½®æ–‡ä»¶
    
    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„
    
    Returns:
        Dict[str, Any]: é…ç½®å­—å…¸
    """
    with open(config_path, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    return config


def setup_device(gpu_id: int = 0) -> str:
    """
    è®¾ç½®è®¡ç®—è®¾å¤‡
    
    Args:
        gpu_id: GPU ID
    
    Returns:
        str: è®¾å¤‡å­—ç¬¦ä¸²ï¼ˆå¦‚ "cuda:0" æˆ– "cpu"ï¼‰
    """
    print("-" * 50)
    if torch.cuda.is_available():
        if gpu_id < torch.cuda.device_count():
            device = f"cuda:{gpu_id}"
            torch.cuda.set_device(gpu_id)
            gpu_name = torch.cuda.get_device_name(gpu_id)
            total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
            print(f"âœ… æˆåŠŸè°ƒç”¨æ˜¾å¡ {gpu_id}: {gpu_name}")
            print(f"ðŸš€ æ˜¾å­˜æ€»é‡: {total_mem:.2f} GB")
        else:
            device = "cuda:0"
            print(f"âš ï¸  æŒ‡å®šçš„GPU {gpu_id}ä¸å­˜åœ¨ï¼Œä½¿ç”¨GPU 0")
    else:
        device = "cpu"
        print("âŒ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°æ˜¾å¡ï¼Œæ­£åœ¨ä½¿ç”¨ CPU (é€Ÿåº¦ä¼šå¾ˆæ…¢)")
    print("-" * 50)
    return device

