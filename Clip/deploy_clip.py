import os

# è®¾ç½® Hugging Face å›½å†…é•œåƒæº (è¿™è¡Œä»£ç å¿…é¡»æ”¾åœ¨ import transformers ä¹‹å‰)
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"


import sys
import cv2
import torch
import numpy as np
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
from pathlib import Path



# è®¾ç½®æ¨¡å‹ç¼“å­˜ç›®å½•åˆ°é¡¹ç›®æ–‡ä»¶å¤¹
PROJECT_ROOT = Path(__file__).parent.parent
MODEL_CACHE_DIR = PROJECT_ROOT / "models" / "clip"
MODEL_CACHE_DIR.mkdir(parents=True, exist_ok=True)
os.environ["TRANSFORMERS_CACHE"] = str(MODEL_CACHE_DIR)
os.environ["HF_HOME"] = str(MODEL_CACHE_DIR)

# ============================
# 1. ç¡¬ä»¶æ£€æµ‹ä¸é…ç½®
# ============================
def setup_device(gpu_id=3):
    """è®¾ç½®è®¡ç®—è®¾å¤‡"""
    print("-" * 50)
    if torch.cuda.is_available():
        if gpu_id < torch.cuda.device_count():
            device = f"cuda:{gpu_id}"
            torch.cuda.set_device(gpu_id)
            gpu_name = torch.cuda.get_device_name(gpu_id)
            total_mem = torch.cuda.get_device_properties(gpu_id).total_memory / 1024**3
            print(f"âœ… æˆåŠŸè°ƒç”¨æ˜¾å¡ {gpu_id}: {gpu_name}")
            print(f"ğŸš€ æ˜¾å­˜æ€»é‡: {total_mem:.2f} GB")
        else:
            device = "cuda:0"
            print(f"âš ï¸  æŒ‡å®šçš„GPU {gpu_id}ä¸å­˜åœ¨ï¼Œä½¿ç”¨GPU 0")
    else:
        device = "cpu"
        print("âŒ è­¦å‘Šï¼šæœªæ£€æµ‹åˆ°æ˜¾å¡ï¼Œæ­£åœ¨ä½¿ç”¨ CPU (é€Ÿåº¦ä¼šå¾ˆæ…¢)")
    print("-" * 50)
    return device

# ============================
# 2. è§†é¢‘å¤„ç†å‡½æ•°
# ============================
def extract_frames_from_video(video_path, num_frames=8):
    """
    ä»è§†é¢‘ä¸­æå–å…³é”®å¸§
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        num_frames: è¦æå–çš„å¸§æ•°
    
    Returns:
        List[PIL.Image]: æå–çš„å¸§å›¾åƒåˆ—è¡¨
    """
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
    
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise ValueError(f"æ— æ³•æ‰“å¼€è§†é¢‘æ–‡ä»¶: {video_path}")
    
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    fps = cap.get(cv2.CAP_PROP_FPS)
    
    # è®¡ç®—é‡‡æ ·é—´éš”
    if total_frames <= num_frames:
        frame_indices = list(range(total_frames))
    else:
        step = total_frames // num_frames
        frame_indices = [i * step for i in range(num_frames)]
    
    frames = []
    for idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = cap.read()
        if ret:
            # è½¬æ¢BGRåˆ°RGB
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            # è½¬æ¢ä¸ºPIL Image
            pil_image = Image.fromarray(frame_rgb)
            frames.append(pil_image)
    
    cap.release()
    print(f"ğŸ“¹ ä»è§†é¢‘ä¸­æå–äº† {len(frames)} å¸§ (æ€»å¸§æ•°: {total_frames}, FPS: {fps:.2f})")
    return frames

# ============================
# 3. CLIPæ¨¡å‹åŠ è½½
# ============================
def load_clip_model(device, model_name="openai/clip-vit-base-patch32"):
    """
    åŠ è½½CLIPæ¨¡å‹
    
    Args:
        device: è®¡ç®—è®¾å¤‡
        model_name: æ¨¡å‹åç§°
    
    Returns:
        model, processor: CLIPæ¨¡å‹å’Œå¤„ç†å™¨
    """
    print(f"â³  æ­£åœ¨ä» HuggingFace ä¸‹è½½å¹¶åŠ è½½ CLIP æ¨¡å‹: {model_name}")
    print(f"ğŸ“  æ¨¡å‹ç¼“å­˜ç›®å½•: {MODEL_CACHE_DIR}")
    
    try:
        # åŠ è½½æ¨¡å‹åˆ°æŒ‡å®šè®¾å¤‡
        model = CLIPModel.from_pretrained(
            model_name,
            cache_dir=str(MODEL_CACHE_DIR),
            use_safetensors=True
        ).to(device)
        model.eval()
        
        # åŠ è½½é¢„å¤„ç†å™¨
        processor = CLIPProcessor.from_pretrained(
            model_name,
            cache_dir=str(MODEL_CACHE_DIR)
        )
        print("ğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸï¼éƒ¨ç½²å®Œæˆã€‚")
        return model, processor
    except Exception as e:
        print(f"ğŸ’¥ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œ: {e}")
        raise

# ============================
# 4. é—®ç­”åŠŸèƒ½
# ============================
def answer_question_with_clip(model, processor, video_frames, question, device, candidate_answers=None):
    """
    ä½¿ç”¨CLIPå›ç­”å…³äºè§†é¢‘çš„é—®é¢˜
    
    Args:
        model: CLIPæ¨¡å‹
        processor: CLIPå¤„ç†å™¨
        video_frames: è§†é¢‘å¸§åˆ—è¡¨
        question: ç”¨æˆ·é—®é¢˜
        device: è®¡ç®—è®¾å¤‡
        candidate_answers: å€™é€‰ç­”æ¡ˆåˆ—è¡¨ï¼ˆå¦‚æœä¸ºNoneï¼Œåˆ™ä½¿ç”¨é»˜è®¤ç­”æ¡ˆï¼‰
    
    Returns:
        str: å›ç­”æ–‡æœ¬
    """
    # å¦‚æœæ²¡æœ‰æä¾›å€™é€‰ç­”æ¡ˆï¼Œä½¿ç”¨é»˜è®¤çš„é—®ç­”æ¨¡æ¿
    if candidate_answers is None:
        # æ ¹æ®é—®é¢˜ç±»å‹ç”Ÿæˆå€™é€‰ç­”æ¡ˆ
        question_lower = question.lower()
        if "real" in question_lower or "fake" in question_lower or "deepfake" in question_lower or "çœŸå®" in question or "è™šå‡" in question or "ä¼ªé€ " in question:
            candidate_answers = [
                "This is a real person speaking in the video.",
                "This is a fake or deepfake video with artificial manipulation.",
                "The video shows authentic human speech and facial movements.",
                "The video contains synthetic or AI-generated content."
            ]
        elif "emotion" in question_lower or "è¡¨æƒ…" in question or "æƒ…ç»ª" in question:
            candidate_answers = [
                "The person appears happy and joyful.",
                "The person appears sad or melancholic.",
                "The person appears angry or frustrated.",
                "The person appears neutral or calm."
            ]
        elif "speaking" in question_lower or "è¯´è¯" in question or "è®²è¯" in question:
            candidate_answers = [
                "The person is speaking clearly and naturally.",
                "The person's mouth movements match the audio.",
                "The person's lip-sync appears synchronized.",
                "There is a mismatch between audio and video."
            ]
        else:
            # é€šç”¨ç­”æ¡ˆ
            candidate_answers = [
                "Yes, this is evident in the video.",
                "No, this is not evident in the video.",
                "The video shows this characteristic clearly.",
                "The video does not show this characteristic."
            ]
    
    # å°†é—®é¢˜ä¸å€™é€‰ç­”æ¡ˆç»„åˆ
    text_inputs = [f"{question} {answer}" for answer in candidate_answers]
    
    # å¤„ç†è§†é¢‘å¸§ï¼ˆå–ç¬¬ä¸€å¸§æˆ–å¹³å‡å¤šå¸§ï¼‰
    if len(video_frames) == 1:
        image = video_frames[0]
    else:
        # å¦‚æœæœ‰å¤šå¸§ï¼Œä½¿ç”¨ç¬¬ä¸€å¸§ä½œä¸ºä»£è¡¨ï¼ˆä¹Ÿå¯ä»¥å¹³å‡å¤šå¸§ï¼‰
        image = video_frames[0]
    
    # é¢„å¤„ç†
    inputs = processor(
        text=text_inputs,
        images=image,
        return_tensors="pt",
        padding=True
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}
    
    # æ¨¡å‹æ¨ç†
    with torch.no_grad():
        outputs = model(**inputs)
        logits_per_image = outputs.logits_per_image
        probs = logits_per_image.softmax(dim=1)
    
    # æ‰¾åˆ°æœ€åŒ¹é…çš„ç­”æ¡ˆ
    best_idx = probs.argmax(dim=1).item()
    confidence = probs[0][best_idx].item()
    best_answer = candidate_answers[best_idx]
    
    return best_answer, confidence, probs.cpu().numpy()[0]

# ============================
# 5. äº¤äº’å¼å¯¹è¯ä¸»å‡½æ•°
# ============================
def interactive_chat(model, processor, device):
    """
    äº¤äº’å¼å¯¹è¯å¾ªç¯
    """
    print("\n" + "=" * 50)
    print("ğŸ¤– CLIP è§†é¢‘é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨")
    print("=" * 50)
    print("ä½¿ç”¨è¯´æ˜:")
    print("  - è¾“å…¥è§†é¢‘è·¯å¾„å’Œé—®é¢˜ï¼Œæ ¼å¼: è§†é¢‘è·¯å¾„|é—®é¢˜")
    print("  - ä¾‹å¦‚: /path/to/video.mp4|è¿™ä¸ªè§†é¢‘æ˜¯çœŸå®çš„è¿˜æ˜¯ä¼ªé€ çš„?")
    print("  - è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
    print("=" * 50 + "\n")
    
    current_video_path = None
    current_frames = None
    
    while True:
        try:
            user_input = input("è¯·è¾“å…¥ (è§†é¢‘è·¯å¾„|é—®é¢˜) æˆ–ç›´æ¥è¾“å…¥é—®é¢˜ (ä½¿ç”¨ä¸Šæ¬¡è§†é¢‘): ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ å†è§ï¼")
                break
            
            if not user_input:
                continue
            
            # è§£æè¾“å…¥
            if '|' in user_input:
                parts = user_input.split('|', 1)
                video_path = parts[0].strip()
                question = parts[1].strip()
                
                # åŠ è½½æ–°è§†é¢‘
                print(f"\nğŸ“¹ æ­£åœ¨åŠ è½½è§†é¢‘: {video_path}")
                current_frames = extract_frames_from_video(video_path, num_frames=8)
                current_video_path = video_path
            else:
                # ä½¿ç”¨ä¸Šæ¬¡çš„è§†é¢‘
                question = user_input
                if current_frames is None:
                    print("âŒ é”™è¯¯: è¯·å…ˆæä¾›è§†é¢‘è·¯å¾„")
                    continue
            
            if not question:
                print("âŒ é”™è¯¯: é—®é¢˜ä¸èƒ½ä¸ºç©º")
                continue
            
            # å›ç­”é—®é¢˜
            print(f"\nâ“ é—®é¢˜: {question}")
            print("ğŸ¤” æ­£åœ¨åˆ†æè§†é¢‘...")
            
            answer, confidence, probs = answer_question_with_clip(
                model, processor, current_frames, question, device
            )
            
            print(f"\nğŸ’¬ CLIPå›ç­”: {answer}")
            print(f"ğŸ“Š ç½®ä¿¡åº¦: {confidence:.2%}")
            print("-" * 50)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()

# ============================
# ä¸»ç¨‹åºå…¥å£
# ============================
def main():
    # è®¾ç½®GPU
    device = setup_device(gpu_id=3)
    
    # åŠ è½½æ¨¡å‹
    model, processor = load_clip_model(device)
    
    # å¯åŠ¨äº¤äº’å¼å¯¹è¯
    interactive_chat(model, processor, device)

if __name__ == "__main__":
    main()
